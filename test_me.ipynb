{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing metric /evaluate\n",
    "from evaluate import load, Metric\n",
    "# Define the evaluation metric\n",
    "metric = load(\"perplexity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 8])\n",
      "torch.Size([3, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a dummy batch of tensors\n",
    "batch_size = 3\n",
    "seq_len = 5\n",
    "num_classes = 8\n",
    "\n",
    "predictions = torch.randn(batch_size, seq_len, num_classes)\n",
    "references = torch.randn(batch_size, seq_len, num_classes)\n",
    "\n",
    "print(predictions.shape)  # torch.Size([3, 5, 8])\n",
    "print(references.shape)  # torch.Size([3, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Module inputs don't match the expected format.\nExpected format: {'predictions': Value(dtype='string', id=None)},\nInput predictions: tensor([[[ 2.3273e+00, -5.3519e-01, -1.2785e+00, -4.1862e-02, -3.3433e-01,\n          -5.5318e-01,  8.4083e-02, -9.8818e-01],\n         [ 1.0188e-02, -2.2751e-01,  1.8050e-01, -1.3902e+00,  1.4129e+00,\n          -2.9251e-01,  1.0297e+00,  7.1149e-01],\n         [-1.1092e+00,  3.0854e-01,  2.8624e-01, -1.2370e+00,  5.0718e-01,\n           1.3862e+00, -7.0663e-02,  1.1982e+00],\n         [ 2.2365e+00, -2.6708e+00,  3.0397e-01,  4.2993e-01, -6.1784e-01,\n           5.7370e-01, -4.9750e-01,  2.4382e-01],\n         [-6.5129e-01,  6.3780e-01, -1.8607e-01, -1.5786e-02, -3.6394e-01,\n           4.8414e-01,  6.7639e-01, -8.1123e-01]],\n\n        [[ 1.0052e+00, -1.4035e+00, -1.8384e+00, -1.8115e+00,  6.3310e-01,\n          -1.0463e+00, -5.1898e-01,  1.5148e-01],\n         [-4.4115e-01, -2.9205e-01,  2.9789e-01,  4.6103e-01,  1.2642e-01,\n           1.1239e+00, -4.5518e-01, -9.1120e-01],\n         [-7.2302e-02,  1.5130e-01, -8.4902e-01,  1.0810e+00,  1.4888e+00,\n           4.4249e-01,  1.0811e-01,  9.2787e-01],\n         [-1.2089e+00, -1.6787e+00, -8.1595e-01,  4.4236e-01, -5.1954e-01,\n          -4.1417e-02,  6.3373e-01,  1.5562e+00],\n         [ 1.7131e-01, -1.1107e+00,  1.2464e-01, -1.5840e+00, -2.3695e-01,\n           1.3830e+00, -1.0756e+00,  9.4802e-01]],\n\n        [[ 8.6343e-01,  5.1408e-01, -4.1687e-01,  5.9132e-01, -9.9444e-01,\n          -1.0034e+00, -3.1581e-01, -3.3502e-01],\n         [ 4.3045e-01, -6.1162e-01,  1.2328e+00,  3.7254e-01,  5.5982e-01,\n          -1.7006e+00, -5.7044e-01, -3.8333e-01],\n         [-4.5406e-01,  1.8573e+00, -1.9913e+00,  1.0388e+00, -1.5333e-03,\n          -5.5408e-02,  3.5681e-01, -1.7408e+00],\n         [ 3.3792e-01, -7.0859e-01,  9.9752e-01,  1.0279e+00,  9.4658e-01,\n           9.6294e-01, -7.5609e-01, -2.0812e-01],\n         [-3.8989e-01,  1.8806e-01, -8.2793e-01,  3.7609e-01,  5.1398e-01,\n          -1.5904e+00, -8.8275e-01, -5.7889e-02]]])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreferences\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\u725561\\llm\\llm\\lib\\site-packages\\evaluate\\module.py:546\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    540\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions and/or references don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match the expected format.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    542\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    543\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    544\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput references: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(references)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m     )\n\u001b[1;32m--> 546\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Module inputs don't match the expected format.\nExpected format: {'predictions': Value(dtype='string', id=None)},\nInput predictions: tensor([[[ 2.3273e+00, -5.3519e-01, -1.2785e+00, -4.1862e-02, -3.3433e-01,\n          -5.5318e-01,  8.4083e-02, -9.8818e-01],\n         [ 1.0188e-02, -2.2751e-01,  1.8050e-01, -1.3902e+00,  1.4129e+00,\n          -2.9251e-01,  1.0297e+00,  7.1149e-01],\n         [-1.1092e+00,  3.0854e-01,  2.8624e-01, -1.2370e+00,  5.0718e-01,\n           1.3862e+00, -7.0663e-02,  1.1982e+00],\n         [ 2.2365e+00, -2.6708e+00,  3.0397e-01,  4.2993e-01, -6.1784e-01,\n           5.7370e-01, -4.9750e-01,  2.4382e-01],\n         [-6.5129e-01,  6.3780e-01, -1.8607e-01, -1.5786e-02, -3.6394e-01,\n           4.8414e-01,  6.7639e-01, -8.1123e-01]],\n\n        [[ 1.0052e+00, -1.4035e+00, -1.8384e+00, -1.8115e+00,  6.3310e-01,\n          -1.0463e+00, -5.1898e-01,  1.5148e-01],\n         [-4.4115e-01, -2.9205e-01,  2.9789e-01,  4.6103e-01,  1.2642e-01,\n           1.1239e+00, -4.5518e-01, -9.1120e-01],\n         [-7.2302e-02,  1.5130e-01, -8.4902e-01,  1.0810e+00,  1.4888e+00,\n           4.4249e-01,  1.0811e-01,  9.2787e-01],\n         [-1.2089e+00, -1.6787e+00, -8.1595e-01,  4.4236e-01, -5.1954e-01,\n          -4.1417e-02,  6.3373e-01,  1.5562e+00],\n         [ 1.7131e-01, -1.1107e+00,  1.2464e-01, -1.5840e+00, -2.3695e-01,\n           1.3830e+00, -1.0756e+00,  9.4802e-01]],\n\n        [[ 8.6343e-01,  5.1408e-01, -4.1687e-01,  5.9132e-01, -9.9444e-01,\n          -1.0034e+00, -3.1581e-01, -3.3502e-01],\n         [ 4.3045e-01, -6.1162e-01,  1.2328e+00,  3.7254e-01,  5.5982e-01,\n          -1.7006e+00, -5.7044e-01, -3.8333e-01],\n         [-4.5406e-01,  1.8573e+00, -1.9913e+00,  1.0388e+00, -1.5333e-03,\n          -5.5408e-02,  3.5681e-01, -1.7408e+00],\n         [ 3.3792e-01, -7.0859e-01,  9.9752e-01,  1.0279e+00,  9.4658e-01,\n           9.6294e-01, -7.5609e-01, -2.0812e-01],\n         [-3.8989e-01,  1.8806e-01, -8.2793e-01,  3.7609e-01,  5.1398e-01,\n          -1.5904e+00, -8.8275e-01, -5.7889e-02]]])"
     ]
    }
   ],
   "source": [
    "metric.add_batch(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ref is [0, 1]\n",
      " preds is [1, 0]\n",
      " ref is [0, 1]\n",
      " preds is [0, 1]\n"
     ]
    }
   ],
   "source": [
    "accuracy = load(\"accuracy\")\n",
    "for refs, preds in zip([[0,1],[0,1]], [[1,0],[0,1]]):\n",
    "  print(f\" ref is {refs}\")\n",
    "  print(f\" preds is {preds}\")\n",
    "  accuracy.add_batch(references=refs, predictions=preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ref is [0, 1]\n",
      " preds is [1, 0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Module inputs don't match the expected format.\nExpected format: {'predictions': Value(dtype='string', id=None)},\nInput predictions: [1, 0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ref is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrefs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m preds is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\u725561\\llm\\llm\\lib\\site-packages\\evaluate\\module.py:546\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    540\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions and/or references don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match the expected format.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    542\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    543\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    544\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput references: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(references)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m     )\n\u001b[1;32m--> 546\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Module inputs don't match the expected format.\nExpected format: {'predictions': Value(dtype='string', id=None)},\nInput predictions: [1, 0]"
     ]
    }
   ],
   "source": [
    "# does not work , gives valueerror\n",
    "metric = load(\"perplexity\")\n",
    "for refs, preds in zip([[0,1],[0,1]], [[1,0],[0,1]]):\n",
    "  print(f\" ref is {refs}\")\n",
    "  print(f\" preds is {preds}\")\n",
    "  metric.add_batch(references=refs, predictions=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ref is [0, 1]\n",
      " preds is [1, 0]\n",
      " ref_str is [0 1]\n",
      " preds is [1 0]\n",
      " ref is [0, 1]\n",
      " preds is [0, 1]\n",
      " ref_str is [0 1]\n",
      " preds is [0 1]\n"
     ]
    }
   ],
   "source": [
    "# for perplexity metric , you need to convert into string\n",
    "from torch import tensor\n",
    "\n",
    "metric = load(\"perplexity\")\n",
    "for refs, preds in zip([[0,1],[0,1]], [[1,0],[0,1]]):\n",
    "  print(f\" ref is {refs}\")\n",
    "  print(f\" preds is {preds}\")\n",
    "  ref_str = str(tensor(refs).numpy())\n",
    "  pred_str = str(tensor(preds).numpy())\n",
    "  print(f\" ref_str is {ref_str}\")\n",
    "  print(f\" preds is {pred_str}\")\n",
    "  metric.add_batch(references=ref_str, predictions=pred_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load(\"perplexity\")\n",
    "# Sample data: references and predictions\n",
    "references = [\"this is a test sentence\", \"another sentence for testing\"]\n",
    "predictions = [\"this is a test sentece\", \"another sentece for testing1\"]\n",
    "\n",
    "# Add batch to metric\n",
    "for ref, pred in zip(references, predictions):\n",
    "    metric.add_batch(references=ref, predictions=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"perplexity\", module_type: \"metric\", features: {'predictions': Value(dtype='string', id=None)}, usage: \"\"\"\n",
       "Args:\n",
       "    model_id (str): model used for calculating Perplexity\n",
       "            NOTE: Perplexity can only be calculated for causal language models.\n",
       "                    This includes models such as gpt2, causal variations of bert,\n",
       "                    causal versions of t5, and more (the full list can be found\n",
       "                    in the AutoModelForCausalLM documentation here:\n",
       "                    https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForCausalLM )\n",
       "\n",
       "    predictions (list of str): input text, each separate text snippet\n",
       "        is one list entry.\n",
       "    batch_size (int): the batch size to run texts through the model. Defaults to 16.\n",
       "    add_start_token (bool): whether to add the start token to the texts,\n",
       "        so the perplexity can include the probability of the first word. Defaults to True.\n",
       "    device (str): device to run on, defaults to 'cuda' when available\n",
       "Returns:\n",
       "    perplexity: dictionary containing the perplexity scores for the texts\n",
       "        in the input list, as well as the mean perplexity. If one of the input texts is\n",
       "        longer than the max input length of the model, then it is truncated to the\n",
       "        max length for the perplexity computation.\n",
       "Examples:\n",
       "    Example 1:\n",
       "        >>> perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
       "        >>> input_texts = [\"lorem ipsum\", \"Happy Birthday!\", \"Bienvenue\"]\n",
       "        >>> results = perplexity.compute(model_id='gpt2',\n",
       "        ...                              add_start_token=False,\n",
       "        ...                              predictions=input_texts) # doctest:+ELLIPSIS\n",
       "        >>> print(list(results.keys()))\n",
       "        ['perplexities', 'mean_perplexity']\n",
       "        >>> print(round(results[\"mean_perplexity\"], 0))\n",
       "        647.0\n",
       "        >>> print(round(results[\"perplexities\"][0], 0))\n",
       "        32.0\n",
       "\n",
       "    Example 2:\n",
       "        >>> from datasets import load_dataset\n",
       "        >>> perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
       "        >>> input_texts = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")[\"text\"][:10] # doctest: +SKIP\n",
       "        >>> input_texts = [s for s in input_texts if s!='']\n",
       "        >>> results = perplexity.compute(model_id='gpt2',\n",
       "        ...                              predictions=input_texts)\n",
       "        >>> print(list(results.keys()))\n",
       "        ['perplexities', 'mean_perplexity']\n",
       "        >>> print(round(results[\"mean_perplexity\"], 2)) # doctest: +SKIP\n",
       "        576.76\n",
       "        >>> print(round(results[\"perplexities\"][0], 2)) # doctest: +SKIP\n",
       "        889.28\n",
       "\"\"\", stored examples: 50)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute perplexity\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2987c274f44eecbc324bd970a19b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  11%|#1        | 62.9M/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f24614556b414e9d72acbd501b6d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8385eca9cb4d46688f3cd507b35ae7ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130907410d4f44f6af24e78151092545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99903778b3343c98ee2dc38a5bf3c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8215438fe8d4825af1397e2bd03456b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e024bbc179cb41ed9bcfea9224676c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = metric.compute(model_id='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'perplexities': [1159.0743408203125,\n",
       "  1109.5885009765625,\n",
       "  718.4129028320312,\n",
       "  503.69384765625,\n",
       "  12868.9052734375,\n",
       "  718.4129028320312,\n",
       "  503.69384765625,\n",
       "  12868.9052734375,\n",
       "  520.3135986328125,\n",
       "  12868.9052734375,\n",
       "  1159.0743408203125,\n",
       "  1623.4150390625,\n",
       "  503.69384765625,\n",
       "  1159.0743408203125,\n",
       "  12868.9052734375,\n",
       "  503.69818115234375,\n",
       "  1623.4150390625,\n",
       "  1117.8475341796875,\n",
       "  1159.0743408203125,\n",
       "  1623.4150390625,\n",
       "  1040.33203125,\n",
       "  1623.4150390625,\n",
       "  520.3135986328125,\n",
       "  1117.8475341796875,\n",
       "  2440.71875,\n",
       "  1159.0743408203125,\n",
       "  1109.5885009765625,\n",
       "  1623.4150390625,\n",
       "  2127.038818359375,\n",
       "  12868.9052734375,\n",
       "  503.69384765625,\n",
       "  1623.4166259765625,\n",
       "  1117.8475341796875,\n",
       "  1159.0743408203125,\n",
       "  1623.4150390625,\n",
       "  1040.33203125,\n",
       "  1623.4150390625,\n",
       "  12868.9052734375,\n",
       "  1561.4671630859375,\n",
       "  2440.71875,\n",
       "  2127.038818359375,\n",
       "  12868.9052734375,\n",
       "  1159.0743408203125,\n",
       "  1623.4150390625,\n",
       "  503.69384765625,\n",
       "  1159.0743408203125,\n",
       "  718.4129028320312,\n",
       "  1117.8614501953125,\n",
       "  1365.3900146484375,\n",
       "  149.02003479003906],\n",
       " 'mean_perplexity': 2823.326787414551}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2823.33\n"
     ]
    }
   ],
   "source": [
    "print(round(results[\"mean_perplexity\"], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
