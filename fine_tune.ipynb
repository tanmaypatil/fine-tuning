{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End-to-end script running the Hugging Face Trainer\n",
    "# for causal language modeling. Based on the Tasks documentation\n",
    "# originally from: https://hf.co/docs/transformers/tasks/language_modeling\n",
    "from accelerate import PartialState\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from util_functions import *\n",
    "from config_train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load, Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4ea2a577f64ec6a9b975a67554bc0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the evaluation metric\n",
    "metric = load(\"perplexity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert/distilgpt2\n"
     ]
    }
   ],
   "source": [
    "print (config_obj['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a base model from configuration.\n",
    "model_name = config_obj['model_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files={\"train\" : \"email.jsonl\",\"test\" : \"email2.jsonl\" })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['type', 'text'],\n",
       "        num_rows: 6\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['type', 'text'],\n",
       "        num_rows: 6\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\u725561\\llm\\llm\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "tokenizer = get_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset for distilbert/distilgpt2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b447e10406b43c8bf1e62f7f3be3856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128fc3d9558541f7b756248b3d4d450b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Tokenizing dataset for {model_name}...\")\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d4f4d3f5d94c6f92c66aef2be9f413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f564d2246440dabb6f781f72a13d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And apply\n",
    "tokenized_dataset = tokenized_dataset.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an efficient collator which dynamically pads\n",
    "# End-of-sequence as the padding token and mlm=False will\n",
    "# use the inputs as labels, shifted to the right by one element\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating model (distilbert/distilgpt2)...\n",
      "Creating training arguments (weights are stored at `results/causal_language_modeling`)...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Instantiating model ({model_name})...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Define the hyperparameters in the TrainingArguments\n",
    "print(\"Creating training arguments (weights are stored at `results/causal_language_modeling`)...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for small dataset - adjusting batch size \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"results/causal_language_modeling\",  # Where weights are stored\n",
    "    learning_rate=1e-5,  # The learning rate during training\n",
    "    per_device_train_batch_size=2,  # Number of samples per batch during training\n",
    "    per_device_eval_batch_size=2,  # Number of samples per batch during evaluation\n",
    "    num_train_epochs=10,  # How many iterations through the dataloaders should be done\n",
    "    weight_decay=0.01,  # Regularization penalization\n",
    "    evaluation_strategy=\"epoch\",  # How often metrics on the evaluation dataset should be computed\n",
    "    save_strategy=\"epoch\",  # When to try and save the best model (such as a step number or every iteration)\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating `Trainer`...\n"
     ]
    }
   ],
   "source": [
    "# Create the `Trainer`, passing in the model and arguments\n",
    "# the datasets to train on, how the data should be collated,\n",
    "# and the method for computing our metrics\n",
    "print(\"Creating `Trainer`...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40ccb3beb804350af6847490406a8d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6263e82e63842ca8408f87910da27d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.271400451660156, 'eval_runtime': 0.113, 'eval_samples_per_second': 8.852, 'eval_steps_per_second': 8.852, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab0b92a0aa448c2a1b1c524d2d34931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.168320178985596, 'eval_runtime': 0.1127, 'eval_samples_per_second': 8.876, 'eval_steps_per_second': 8.876, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3f3545383749b5ba72cc1a0b966f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.079220294952393, 'eval_runtime': 0.1161, 'eval_samples_per_second': 8.612, 'eval_steps_per_second': 8.612, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330d278bc7894196b21e7a0f24a08887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.005306720733643, 'eval_runtime': 0.1002, 'eval_samples_per_second': 9.976, 'eval_steps_per_second': 9.976, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7797dbf2b94046559694192423693f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.9435272216796875, 'eval_runtime': 0.0973, 'eval_samples_per_second': 10.275, 'eval_steps_per_second': 10.275, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913c654ddae74313bf8e7c73cee21ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.894034385681152, 'eval_runtime': 0.1202, 'eval_samples_per_second': 8.322, 'eval_steps_per_second': 8.322, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dfa412880b44af59259f0a597a49a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.854833602905273, 'eval_runtime': 0.1192, 'eval_samples_per_second': 8.388, 'eval_steps_per_second': 8.388, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c74c88d8a947c2bff218758acb3cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.825794696807861, 'eval_runtime': 0.147, 'eval_samples_per_second': 6.805, 'eval_steps_per_second': 6.805, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4882a7db7448415a80c0584e41712974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.806646823883057, 'eval_runtime': 0.1153, 'eval_samples_per_second': 8.672, 'eval_steps_per_second': 8.672, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54d21adaf7945998dc7e00bc9cf698d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.7971415519714355, 'eval_runtime': 0.132, 'eval_samples_per_second': 7.575, 'eval_steps_per_second': 7.575, 'epoch': 10.0}\n",
      "{'train_runtime': 22.177, 'train_samples_per_second': 0.451, 'train_steps_per_second': 0.451, 'train_loss': 5.16423225402832, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=5.16423225402832, metrics={'train_runtime': 22.177, 'train_samples_per_second': 0.451, 'train_steps_per_second': 0.451, 'total_flos': 267931238400.0, 'train_loss': 5.16423225402832, 'epoch': 10.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate training\n",
    "print(\"Training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing inference\n",
    "text = \"p2go has mostly \"\n",
    "# We need to tokenize the inputs and turn them to PyTorch tensors\n",
    "encoded_input = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded_input = { 'input_ids' : encoded_input.input_ids , 'attention_mask' : encoded_input.attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To move the batch to the right device automatically, use `PartialState().device`\n",
    "# which will always work no matter the environment\n",
    "# encoded_input = encoded_input['input_ids'].to(PartialState().device)\n",
    "# encoded_input.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing inference...\n"
     ]
    }
   ],
   "source": [
    "# Then we can perform inference via `model.generate`:\n",
    "print(\"Performing inference...\")\n",
    "outputs = model.generate(encoded_input['input_ids'], max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: ['p2go has mostly irc servers, the server has been a little much smaller. It can\\'t run all the servers. It\\'s only a bit slower than a normal web server. But I mean how fast do we need to run servers? I just have to give my name. This is the code in the README file of the project. It goes as follows:\\n\\n\\nimport { \"git clone https://github.com/xrudy-sip/xrudy-sip/']\n"
     ]
    }
   ],
   "source": [
    "# Finally, decode our outputs\n",
    "print(f\"Prediction: {tokenizer.batch_decode(outputs, skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
