{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup, set_seed\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    LoraConfig,\n",
    "    PeftType,\n",
    "    PrefixTuningConfig,\n",
    "    PromptEncoderConfig,\n",
    ")\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"text\", data_files={\"train\": [\"dependency_score.txt\"],\"test\": [\"dependency_score_test.txt\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 51\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "model_name_or_path = \"openai-community/gpt2\"\n",
    "peft_type = PeftType.LORA\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if any(k in model_name_or_path for k in (\"gpt\", \"opt\", \"bloom\")):\n",
    "    padding_side = \"left\"\n",
    "else:\n",
    "    padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=padding_side)\n",
    "if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # max_length=None => use the model max length (it's actually the default)\n",
    "    outputs = tokenizer(examples[\"text\"], padding=True,truncation=True)\n",
    "    outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 51\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it has 51 rows\n",
    "(tokenized_datasets['train']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = tokenizer.decode(tokenized_datasets['train'][0]['input_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background\n"
     ]
    }
   ],
   "source": [
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized_datasets['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " decoded : Background\n",
      " decoded : Software product developed by a large organisation have inter team dependencies.\n",
      " decoded : These dependencies stall the flow of execution and paralyse the teams. Following teams are the actors in a cross team collaborated product\n",
      " decoded : \n",
      " decoded : Team which develops application, focuses on functional aspects\n",
      " decoded : Infra team which provides infrastructure such as security, authentication, resilliency\n",
      " decoded : Devops team which provides access to resources such as kubernetes cluster, IDP's, active directory etc.\n",
      " decoded : Application development team gets stuck because of dependencies on infra and devops team. During execution, stakeholders (such as program managers, engineering directors ) are looking for input on the epics and requirement which has higher dependency on other teams.\n",
      " decoded : \n",
      " decoded : This is an attempt to look at calculating dependency score of a epic. This will help managers to focus on high dependency epics also compare one product increment to another product increment.\n",
      " decoded : \n",
      " decoded : How to calculate score\n",
      " decoded : Current method of implementation is to use weighted score of following parameters to arrive at dependency score\n",
      " decoded : \n",
      " decoded : Requirement effort - 10% weightage\n",
      " decoded : Dependency effort - 10 % weightage\n",
      " decoded : Product priority - 50% weightage\n",
      " decoded : Sprint arrival - 30% weightage\n",
      " decoded : Score calculation - input scaling\n",
      " decoded : All the input variables ( such as requirement effort) is scaled to value between 1 and 5 so as to have uniformity. Final dependency score would between less than 5. Higher the score and higher the dependency of a requirement or epic on other teams\n",
      " decoded : \n",
      " decoded : Variables\n",
      " decoded : Requirement effort\n",
      " decoded : Effort in person days of the epic under question. It could be between 0 to 200 person days. Lower and upper bounds are configurable. This value is scaled down to be between 0 to 5.\n",
      " decoded : Dependency effort\n",
      " decoded : Effort in person days of dependency effort.For example if infra work needs to be done prior to complete functional work,then infra epic is a dependency. Value is scaled down to be between 0 to 5\n",
      " decoded : Product priority\n",
      " decoded : Importance of epic from product management point of view. A value between 1 and 2. Decimal values are allowed. Dependency score is reversely proportional to product priority in the way value is assigned.\n",
      " decoded : sprint arrival\n",
      " decoded : If dependency is coming late in the product increment, then dependency score would be higher\n",
      " decoded : How to use the repository\n",
      " decoded : Clone the repo. Go to ui directory and run the ui_graph.py. It will start a web ui ( gradio based). UI will ask for 2 inputs\n",
      " decoded : \n",
      " decoded : Inputs\n",
      " decoded : Xls containing epics. This will be one of the xls from requirement folder\n",
      " decoded : number of top nodes as per dependency node. If you give value of 2. It will highlight top 2 nodes ( based on decreasing value of dependency score) in orange color\n",
      " decoded : Outputs\n",
      " decoded : Tree based graph of epics showing dependency score\n",
      " decoded : Dataframe containing calculation on how the dependency score is arrived.\n",
      " decoded : Deep dive into calculation\n",
      " decoded : A deep dive into calculation of the score.\n",
      " decoded : If you follow requirement_2.xlsx in requirement directory, first dependency is GDB-100.\n",
      " decoded : Dependency score calculation will be as follows\n",
      " decoded : \n",
      " decoded : variable name\tvalue\tnormalised value\tweight\tscore\n",
      " decoded : requirement effort\t100\t2.55\t10\t0.255\n",
      " decoded : dependency effort\t20\t0.59\t10\t.059\n",
      " decoded : product priority\t1.10\t4.51\t50\t2.255\n",
      " decoded : arrival sprint\t4\t5\t30\t1.5\n",
      " decoded : All the values are normalised to score between 0 to 5.\n",
      " decoded : Dependency score for GDB-100 = 0.255 +.059 + 2.255 + 1.5 = 4.069 ~= 4.07\n"
     ]
    }
   ],
   "source": [
    "# try decode the encoded 51 rows . See if you see original text\n",
    "for row in tokenized_datasets['train']:\n",
    "  tokens = row['input_ids']\n",
    "  decoded_text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "  print(f\" decoded : {decoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    return tokenizer.pad(examples, padding=\"longest\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate dataloaders.\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, collate_fn=collate_fn, batch_size=batch_size)\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"test\"], shuffle=False, collate_fn=collate_fn, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original LORA config copied from hugging face repo .\n",
    "# task type changed to Seq generation \n",
    "peft_config = LoraConfig(task_type=\"SEQ_GEN\", inference_mode=False, r=8, lora_alpha=16, lora_dropout=0.1)\n",
    "lr = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.2364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, return_dict=True)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(params=model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate scheduler\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0.06 * (len(train_dataloader) * num_epochs),\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load, Metric\n",
    "# Define the evaluation metric\n",
    "metric = load(\"perplexity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 26/26 [00:08<00:00,  3.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[labels] tensor([[ 1890, 20203,  4776, 17952,   837,  4511,  3463,   496,  2925,   284,\n",
      "          1720,  8475,   764],\n",
      "        [ 2504,   561,  1612,  2440,   262,  1720,  8475,    11,  2440,   262,\n",
      "         20203,  4776,   764]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a80a30ea3c34ba7ba6c5d79546aa397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'perplexities': [1469.541259765625, 716.8955688476562], 'mean_perplexity': 1093.2184143066406}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:08<00:00,  2.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[labels] tensor([[ 1890, 20203,  4776, 17952,   837,  4511,  3463,   496,  2925,   284,\n",
      "          1720,  8475,   764],\n",
      "        [ 2504,   561,  1612,  2440,   262,  1720,  8475,    11,  2440,   262,\n",
      "         20203,  4776,   764]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5a872498f54bf2bdb8ed85558f7feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: {'perplexities': [1469.541259765625, 716.8955688476562], 'mean_perplexity': 1093.2184143066406}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:08<00:00,  3.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[labels] tensor([[ 1890, 20203,  4776, 17952,   837,  4511,  3463,   496,  2925,   284,\n",
      "          1720,  8475,   764],\n",
      "        [ 2504,   561,  1612,  2440,   262,  1720,  8475,    11,  2440,   262,\n",
      "         20203,  4776,   764]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245f11db9e1344d089a8a7a4fb35b48c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: {'perplexities': [1469.541259765625, 716.8955688476562], 'mean_perplexity': 1093.2184143066406}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:08<00:00,  2.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[labels] tensor([[ 1890, 20203,  4776, 17952,   837,  4511,  3463,   496,  2925,   284,\n",
      "          1720,  8475,   764],\n",
      "        [ 2504,   561,  1612,  2440,   262,  1720,  8475,    11,  2440,   262,\n",
      "         20203,  4776,   764]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58910885fd584b7594690076c4db78ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: {'perplexities': [1469.541259765625, 716.8955688476562], 'mean_perplexity': 1093.2184143066406}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:09<00:00,  2.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[labels] tensor([[ 1890, 20203,  4776, 17952,   837,  4511,  3463,   496,  2925,   284,\n",
      "          1720,  8475,   764],\n",
      "        [ 2504,   561,  1612,  2440,   262,  1720,  8475,    11,  2440,   262,\n",
      "         20203,  4776,   764]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4dae93f1a54a1b8d8e207e9a5349cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: {'perplexities': [1469.541259765625, 716.8955688476562], 'mean_perplexity': 1093.2184143066406}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:09<00:00,  2.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[labels] tensor([[ 1890, 20203,  4776, 17952,   837,  4511,  3463,   496,  2925,   284,\n",
      "          1720,  8475,   764],\n",
      "        [ 2504,   561,  1612,  2440,   262,  1720,  8475,    11,  2440,   262,\n",
      "         20203,  4776,   764]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f16a4e49b5b4bf29c20950748650135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5: {'perplexities': [1469.541259765625, 716.8955688476562], 'mean_perplexity': 1093.2184143066406}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:09<00:00,  2.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[labels] tensor([[ 1890, 20203,  4776, 17952,   837,  4511,  3463,   496,  2925,   284,\n",
      "          1720,  8475,   764],\n",
      "        [ 2504,   561,  1612,  2440,   262,  1720,  8475,    11,  2440,   262,\n",
      "         20203,  4776,   764]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905e961d41014548b571440fc473755a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6: {'perplexities': [1469.541259765625, 716.8955688476562], 'mean_perplexity': 1093.2184143066406}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:09<00:00,  2.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[labels] tensor([[ 1890, 20203,  4776, 17952,   837,  4511,  3463,   496,  2925,   284,\n",
      "          1720,  8475,   764],\n",
      "        [ 2504,   561,  1612,  2440,   262,  1720,  8475,    11,  2440,   262,\n",
      "         20203,  4776,   764]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d63d10d78254193acac54ab01dae16b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7: {'perplexities': [1469.541259765625, 716.8955688476562], 'mean_perplexity': 1093.2184143066406}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:09<00:00,  2.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[labels] tensor([[ 1890, 20203,  4776, 17952,   837,  4511,  3463,   496,  2925,   284,\n",
      "          1720,  8475,   764],\n",
      "        [ 2504,   561,  1612,  2440,   262,  1720,  8475,    11,  2440,   262,\n",
      "         20203,  4776,   764]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a378a8d3309a4fd9ac52b0b547c58bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8: {'perplexities': [1469.541259765625, 716.8955688476562], 'mean_perplexity': 1093.2184143066406}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:09<00:00,  2.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[labels] tensor([[ 1890, 20203,  4776, 17952,   837,  4511,  3463,   496,  2925,   284,\n",
      "          1720,  8475,   764],\n",
      "        [ 2504,   561,  1612,  2440,   262,  1720,  8475,    11,  2440,   262,\n",
      "         20203,  4776,   764]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba73141e18d4c31a39b65961c81feee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9: {'perplexities': [1469.541259765625, 716.8955688476562], 'mean_perplexity': 1093.2184143066406}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:09<00:00,  2.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[labels] tensor([[ 1890, 20203,  4776, 17952,   837,  4511,  3463,   496,  2925,   284,\n",
      "          1720,  8475,   764],\n",
      "        [ 2504,   561,  1612,  2440,   262,  1720,  8475,    11,  2440,   262,\n",
      "         20203,  4776,   764]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e311a70fdd9b4c2a8bc9dc015f2da7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10: {'perplexities': [1469.541259765625, 716.8955688476562], 'mean_perplexity': 1093.2184143066406}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:09<00:00,  2.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[labels] tensor([[ 1890, 20203,  4776, 17952,   837,  4511,  3463,   496,  2925,   284,\n",
      "          1720,  8475,   764],\n",
      "        [ 2504,   561,  1612,  2440,   262,  1720,  8475,    11,  2440,   262,\n",
      "         20203,  4776,   764]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d64a6c65394ca3a838a3ea109b6e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11: {'perplexities': [1469.541259765625, 716.8955688476562], 'mean_perplexity': 1093.2184143066406}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:10<00:00,  2.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[labels] tensor([[ 1890, 20203,  4776, 17952,   837,  4511,  3463,   496,  2925,   284,\n",
      "          1720,  8475,   764],\n",
      "        [ 2504,   561,  1612,  2440,   262,  1720,  8475,    11,  2440,   262,\n",
      "         20203,  4776,   764]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62aed189b73e4e2ebc0cdb40b433c3ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12: {'perplexities': [1469.541259765625, 716.8955688476562], 'mean_perplexity': 1093.2184143066406}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:09<00:00,  2.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[labels] tensor([[ 1890, 20203,  4776, 17952,   837,  4511,  3463,   496,  2925,   284,\n",
      "          1720,  8475,   764],\n",
      "        [ 2504,   561,  1612,  2440,   262,  1720,  8475,    11,  2440,   262,\n",
      "         20203,  4776,   764]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b658ae43f36540c3a3e3b922289ffcba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13: {'perplexities': [1469.541259765625, 716.8955688476562], 'mean_perplexity': 1093.2184143066406}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:09<00:00,  2.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[labels] tensor([[ 1890, 20203,  4776, 17952,   837,  4511,  3463,   496,  2925,   284,\n",
      "          1720,  8475,   764],\n",
      "        [ 2504,   561,  1612,  2440,   262,  1720,  8475,    11,  2440,   262,\n",
      "         20203,  4776,   764]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e67cbe8b1f4760a00ccf2064d1b3db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14: {'perplexities': [1469.541259765625, 716.8955688476562], 'mean_perplexity': 1093.2184143066406}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:09<00:00,  2.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[labels] tensor([[ 1890, 20203,  4776, 17952,   837,  4511,  3463,   496,  2925,   284,\n",
      "          1720,  8475,   764],\n",
      "        [ 2504,   561,  1612,  2440,   262,  1720,  8475,    11,  2440,   262,\n",
      "         20203,  4776,   764]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6dfcc044654b0e808df1211339f8ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15: {'perplexities': [1469.541259765625, 716.8955688476562], 'mean_perplexity': 1093.2184143066406}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:14<00:00,  1.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[labels] tensor([[ 1890, 20203,  4776, 17952,   837,  4511,  3463,   496,  2925,   284,\n",
      "          1720,  8475,   764],\n",
      "        [ 2504,   561,  1612,  2440,   262,  1720,  8475,    11,  2440,   262,\n",
      "         20203,  4776,   764]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eebcdc5029f41b086814b5429348f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16: {'perplexities': [1469.541259765625, 716.8955688476562], 'mean_perplexity': 1093.2184143066406}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:15<00:00,  1.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[labels] tensor([[ 1890, 20203,  4776, 17952,   837,  4511,  3463,   496,  2925,   284,\n",
      "          1720,  8475,   764],\n",
      "        [ 2504,   561,  1612,  2440,   262,  1720,  8475,    11,  2440,   262,\n",
      "         20203,  4776,   764]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e630ecff64f64be09d8d7f3b9b1dfd7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17: {'perplexities': [1469.541259765625, 716.8955688476562], 'mean_perplexity': 1093.2184143066406}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:15<00:00,  1.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[labels] tensor([[ 1890, 20203,  4776, 17952,   837,  4511,  3463,   496,  2925,   284,\n",
      "          1720,  8475,   764],\n",
      "        [ 2504,   561,  1612,  2440,   262,  1720,  8475,    11,  2440,   262,\n",
      "         20203,  4776,   764]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42db6235b95643a5b42ac09035cc1e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18: {'perplexities': [1469.541259765625, 716.8955688476562], 'mean_perplexity': 1093.2184143066406}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:15<00:00,  1.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[labels] tensor([[ 1890, 20203,  4776, 17952,   837,  4511,  3463,   496,  2925,   284,\n",
      "          1720,  8475,   764],\n",
      "        [ 2504,   561,  1612,  2440,   262,  1720,  8475,    11,  2440,   262,\n",
      "         20203,  4776,   764]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c479876b3514bfc9caee0a0d81e3d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19: {'perplexities': [1469.541259765625, 716.8955688476562], 'mean_perplexity': 1093.2184143066406}\n"
     ]
    }
   ],
   "source": [
    "model.to(torch.device(\"cpu\"))\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch.to(torch.device(\"cpu\"))\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        batch.to(torch.device(\"cpu\"))\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        predicted_labels = [tokenizer.decode(label, skip_special_tokens=True) for label in predictions]\n",
    "        print( f\"batch[labels] {batch['labels']}\")\n",
    "        labels_text = [tokenizer.decode(label, skip_special_tokens=True) for label in batch[\"labels\"]]\n",
    "        metric.add_batch(\n",
    "            predictions=predicted_labels,\n",
    "            references=labels_text,\n",
    "        )\n",
    "    eval_metric = metric.compute(model_id=model_name_or_path)\n",
    "    print(f\"epoch {epoch}:\", eval_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
